{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Spark Catalog\n",
    "\n",
    "Let us get an overview of Spark Catalog to manage Spark Metastore tables as well as temporary views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "* Let us say `spark` is of type `SparkSession`. There is an attribute as part of `spark` called as catalog and it is of type pyspark.sql.catalog.Catalog.\n",
    "* We can access catalog using `spark.catalog`.\n",
    "* We can permanently or temporarily create tables or views on top of data in a Data Frame.\n",
    "* Metadata such as table names, column names, data types etc for the permanent tables or views will be stored in Metastore. We can access the metadata using `spark.catalog` which is exposed as part of SparkSession object.\n",
    "* `spark.catalog` also provide us the details related to temporary views that are being created. Metadata of these temporary views will not be stored in Spark Metastore.\n",
    "* Permanent tables are typically created using databases in spark metastore. If not specified, the tables will be created in **default** database.\n",
    "* There are several methods that are part of `spark.catalog`. We will explore them in the later topics.\n",
    "* Following are some of the tasks that can be performed using `spark.catalog` object.\n",
    "  * Check current database and switch to different databases.\n",
    "  * Create permanent table in metastore.\n",
    "  * Create or drop temporary views.\n",
    "  * Register functions.\n",
    "* All the above tasks can be performed using SQL style commands passed to `spark.sql`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.spark.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Spark Metastore'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.catalog.Catalog at 0x7fbe54f639e8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Catalog in module pyspark.sql.catalog object:\n",
      "\n",
      "class Catalog(builtins.object)\n",
      " |  User-facing catalog API, accessible through `SparkSession.catalog`.\n",
      " |  \n",
      " |  This is a thin wrapper around its Scala implementation org.apache.spark.sql.catalog.Catalog.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sparkSession)\n",
      " |      Create a new Catalog that wraps the underlying JVM object.\n",
      " |  \n",
      " |  cacheTable(self, tableName)\n",
      " |      Caches the specified table in-memory.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  clearCache(self)\n",
      " |      Removes all cached tables from the in-memory cache.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  createExternalTable(self, tableName, path=None, source=None, schema=None, **options)\n",
      " |      Creates a table based on the dataset in a data source.\n",
      " |      \n",
      " |      It returns the DataFrame associated with the external table.\n",
      " |      \n",
      " |      The data source is specified by the ``source`` and a set of ``options``.\n",
      " |      If ``source`` is not specified, the default data source configured by\n",
      " |      ``spark.sql.sources.default`` will be used.\n",
      " |      \n",
      " |      Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
      " |      created external table.\n",
      " |      \n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  createTable(self, tableName, path=None, source=None, schema=None, **options)\n",
      " |      Creates a table based on the dataset in a data source.\n",
      " |      \n",
      " |      It returns the DataFrame associated with the table.\n",
      " |      \n",
      " |      The data source is specified by the ``source`` and a set of ``options``.\n",
      " |      If ``source`` is not specified, the default data source configured by\n",
      " |      ``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is\n",
      " |      created from the data at the given path. Otherwise a managed table is created.\n",
      " |      \n",
      " |      Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
      " |      created table.\n",
      " |      \n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      .. versionadded:: 2.2\n",
      " |  \n",
      " |  currentDatabase(self)\n",
      " |      Returns the current default database in this session.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  dropGlobalTempView(self, viewName)\n",
      " |      Drops the global temporary view with the given view name in the catalog.\n",
      " |      If the view has been cached before, then it will also be uncached.\n",
      " |      Returns true if this view is dropped successfully, false otherwise.\n",
      " |      \n",
      " |      >>> spark.createDataFrame([(1, 1)]).createGlobalTempView(\"my_table\")\n",
      " |      >>> spark.table(\"global_temp.my_table\").collect()\n",
      " |      [Row(_1=1, _2=1)]\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"my_table\")\n",
      " |      >>> spark.table(\"global_temp.my_table\") # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      AnalysisException: ...\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  dropTempView(self, viewName)\n",
      " |      Drops the local temporary view with the given view name in the catalog.\n",
      " |      If the view has been cached before, then it will also be uncached.\n",
      " |      Returns true if this view is dropped successfully, false otherwise.\n",
      " |      \n",
      " |      Note that, the return type of this method was None in Spark 2.0, but changed to Boolean\n",
      " |      in Spark 2.1.\n",
      " |      \n",
      " |      >>> spark.createDataFrame([(1, 1)]).createTempView(\"my_table\")\n",
      " |      >>> spark.table(\"my_table\").collect()\n",
      " |      [Row(_1=1, _2=1)]\n",
      " |      >>> spark.catalog.dropTempView(\"my_table\")\n",
      " |      >>> spark.table(\"my_table\") # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      AnalysisException: ...\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  isCached(self, tableName)\n",
      " |      Returns true if the table is currently cached in-memory.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  listColumns(self, tableName, dbName=None)\n",
      " |      Returns a list of columns for the given table/view in the specified database.\n",
      " |      \n",
      " |      If no database is specified, the current database is used.\n",
      " |      \n",
      " |      Note: the order of arguments here is different from that of its JVM counterpart\n",
      " |      because Python does not support method overloading.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  listDatabases(self)\n",
      " |      Returns a list of databases available across all sessions.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  listFunctions(self, dbName=None)\n",
      " |      Returns a list of functions registered in the specified database.\n",
      " |      \n",
      " |      If no database is specified, the current database is used.\n",
      " |      This includes all temporary functions.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  listTables(self, dbName=None)\n",
      " |      Returns a list of tables/views in the specified database.\n",
      " |      \n",
      " |      If no database is specified, the current database is used.\n",
      " |      This includes all temporary views.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  recoverPartitions(self, tableName)\n",
      " |      Recovers all the partitions of the given table and update the catalog.\n",
      " |      \n",
      " |      Only works with a partitioned table, and not a view.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.1\n",
      " |  \n",
      " |  refreshByPath(self, path)\n",
      " |      Invalidates and refreshes all the cached data (and the associated metadata) for any\n",
      " |      DataFrame that contains the given data source path.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |  \n",
      " |  refreshTable(self, tableName)\n",
      " |      Invalidates and refreshes all the cached data and metadata of the given table.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  registerFunction(self, name, f, returnType=None)\n",
      " |      An alias for :func:`spark.udf.register`.\n",
      " |      See :meth:`pyspark.sql.UDFRegistration.register`.\n",
      " |      \n",
      " |      .. note:: Deprecated in 2.3.0. Use :func:`spark.udf.register` instead.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  setCurrentDatabase(self, dbName)\n",
      " |      Sets the current default database in this session.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  uncacheTable(self, tableName)\n",
      " |      Removes the specified table from the in-memory cache.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
