{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Schema for Tables using StructType\n",
    "\n",
    "When we want to create a table using `spark.catalog.createTable` or using `spark.catalog.createExternalTable`, we need to specify Schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Schema can be inferred or we can pass schema using `StructType` object while creating the table..\n",
    "* `StructType` takes list of objects of type `StructField`.\n",
    "* `StructField` is built using column name and data type. All the data types are available under `pyspark.sql.types`.\n",
    "* We need to pass table name and schema for `spark.catalog.createTable`.\n",
    "* We have to pass path along with name and schema for `spark.catalog.createExternalTable`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.spark.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Spark Metastore'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions', '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Let us perform tasks to create empty table using `spark.catalog.createTable` or using `spark.catalog.createExternalTable`.\n",
    "\n",
    "* Create database **{username}_hr_db** and table **employees** with following fields. Let us create Database first and then we will see how to create table.\n",
    "  * employee_id of type Integer\n",
    "  * first_name of type String\n",
    "  * last_name of type String\n",
    "  * salary of type Float\n",
    "  * nationality of type String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {username}_hr_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f\"{username}_hr_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark_hr_db'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtableName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Creates a table based on the dataset in a data source.\n",
       "\n",
       "It returns the DataFrame associated with the table.\n",
       "\n",
       "The data source is specified by the ``source`` and a set of ``options``.\n",
       "If ``source`` is not specified, the default data source configured by\n",
       "``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is\n",
       "created from the data at the given path. Otherwise a managed table is created.\n",
       "\n",
       "Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
       "created table.\n",
       "\n",
       ":return: :class:`DataFrame`\n",
       "\n",
       ".. versionadded:: 2.2\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/catalog.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.catalog.createTable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Build StructType object using StructField list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, \\\n",
    "    IntegerType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesSchema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType()),\n",
    "    StructField(\"first_name\", StringType()),\n",
    "    StructField(\"last_name\", StringType()),\n",
    "    StructField(\"salary\", FloatType()),\n",
    "    StructField(\"nationality\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(employee_id,IntegerType,true),StructField(first_name,StringType,true),StructField(last_name,StringType,true),StructField(salary,FloatType,true),StructField(nationality,StringType,true)))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(employeesSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'struct<employee_id:int,first_name:string,last_name:string,salary:float,nationality:string>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesSchema.simpleString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('DROP TABLE IF EXISTS employees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create table by passing StructType object as schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>nationality</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+----------+---------+------+-----------+\n",
       "|employee_id|first_name|last_name|salary|nationality|\n",
       "+-----------+----------+---------+------+-----------+\n",
       "+-----------+----------+---------+------+-----------+"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.createTable(\"employees\", schema=employeesSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* List the tables from database created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employees', database='spark_hr_db', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='employee_id', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='first_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='last_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='salary', description=None, dataType='float', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='nationality', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns('employees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                          |comment|\n",
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "|employee_id                 |int                                                                                |null   |\n",
      "|first_name                  |string                                                                             |null   |\n",
      "|last_name                   |string                                                                             |null   |\n",
      "|salary                      |float                                                                              |null   |\n",
      "|nationality                 |string                                                                             |null   |\n",
      "|                            |                                                                                   |       |\n",
      "|# Detailed Table Information|                                                                                   |       |\n",
      "|Database                    |spark_hr_db                                                                    |       |\n",
      "|Table                       |employees                                                                          |       |\n",
      "|Owner                       |spark                                                                          |       |\n",
      "|Created Time                |Fri Mar 12 11:13:36 EST 2021                                                       |       |\n",
      "|Last Access                 |Wed Dec 31 19:00:00 EST 1969                                                       |       |\n",
      "|Created By                  |Spark 2.4.7                                                                        |       |\n",
      "|Type                        |MANAGED                                                                            |       |\n",
      "|Provider                    |parquet                                                                            |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1615565616]                                                 |       |\n",
      "|Location                    |hdfs://m01.spark.com:9000/user/spark/warehouse/spark_hr_db.db/employees|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                        |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat                      |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat                     |       |\n",
      "|Storage Properties          |[serialization.format=1]                                                           |       |\n",
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE FORMATTED employees').show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
